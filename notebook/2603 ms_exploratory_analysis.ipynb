{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color = \"#507bbf\"\n",
    "plt.style.use(\"bmh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {\n",
    "    \"stfmjob\":  \"How satisfied are you in your main job\",\n",
    "    \"trdawrk\":  \"Too tired after work to enjoy things like doing at home, how often\",\n",
    "    \"jbprtfp\":  \"Job prevents you from giving time to partner/family, how often\",\n",
    "    \"pfmfdjba\": \"Partner/family fed up with pressure of your job, how often\",\n",
    "    \"dcsfwrka\": \"Current job: can decide time start/finish work\",\n",
    "    \"wrkhome\":  \"Work from home or place of choice, how often\",\n",
    "    \"wrklong\":  \"Employees expected to work overtime, how often\",\n",
    "    \"wrkresp\":  \"Employees expected to be responsive outside working hours, how often\",\n",
    "    \"health\":   \"Subjective general health\",\n",
    "    \"stfeco\":   \"How satisfied with present state of economy in country\",\n",
    "    \"hhmmb\":    \"Number of people living regularly as member of household\",\n",
    "    \"hincfel\":  \"Feeling about household's income nowadays\",\n",
    "    \"trstplc\":  \"Trust in the police\",\n",
    "    \"sclmeet\":  \"How often socially meet with friends, relatives or colleagues\",\n",
    "    \"hlthhmp\":  \"Hampered in daily activities by illness/disability/infirmity/mental problem\",\n",
    "    \"sclact\":   \"Take part in social activities compared to others of same age\",\n",
    "    \"iphlppl\":  \"Important to help people and care for others well-being\",\n",
    "    \"ipsuces\":  \"Important to be successful and that people recognise achievements\",\n",
    "    \"ipstrgv\":  \"Important that government is strong and ensures safety\",\n",
    "    \"gndr\"   :  \"Gender\",\n",
    "    \"cntry\"  :  \"Country\",\n",
    "    \"happy\":    \"Happiness\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31784/4004473116.py:1: DtypeWarning: Columns (82,88,90) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/20240319_ESS10_manually-filtered_arthurcornelio88.csv\"\n",
    ").reset_index(drop=True)\n",
    "data = data.drop(\"Unnamed: 0\", axis=1)\n",
    "data = data[~data[\"happy\"].isin([77, 88, 99])]\n",
    "\n",
    "with open(\"../data/features_table.json\", 'r') as file:\n",
    "    features_tables = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xpred\n",
    "data = pd.DataFrame([np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\"DE\",1,1])], \\\n",
    "    columns=[\"stfmjob\",\"trdawrk\",\"jbprtfp\", \"pfmfdjba\", \"dcsfwrka\", \"wrkhome\", \\\n",
    "        \"wrklong\", \"wrkresp\", \"health\",\"stfeco\",\"hhmmb\",\"hincfel\", \"trstplc\", \\\n",
    "        \"sclmeet\", \"hlthhmp\", \"sclact\",\"iphlppl\", \"ipsuces\", \"ipstrgv\", \"gndr\", \\\n",
    "       \"cntry\", \"happy\",\"happy_reduced\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stfmjob</th>\n",
       "      <th>trdawrk</th>\n",
       "      <th>jbprtfp</th>\n",
       "      <th>pfmfdjba</th>\n",
       "      <th>dcsfwrka</th>\n",
       "      <th>wrkhome</th>\n",
       "      <th>wrklong</th>\n",
       "      <th>wrkresp</th>\n",
       "      <th>health</th>\n",
       "      <th>stfeco</th>\n",
       "      <th>...</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>hlthhmp</th>\n",
       "      <th>sclact</th>\n",
       "      <th>iphlppl</th>\n",
       "      <th>ipsuces</th>\n",
       "      <th>ipstrgv</th>\n",
       "      <th>gndr</th>\n",
       "      <th>cntry</th>\n",
       "      <th>happy</th>\n",
       "      <th>happy_reduced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>DE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  stfmjob trdawrk jbprtfp pfmfdjba dcsfwrka wrkhome wrklong wrkresp health  \\\n",
       "0       1       1       1        1        1       1       1       1      1   \n",
       "\n",
       "  stfeco  ... sclmeet hlthhmp sclact iphlppl ipsuces ipstrgv gndr cntry happy  \\\n",
       "0      1  ...       1       1      1       1       1       1    1    DE     1   \n",
       "\n",
       "  happy_reduced  \n",
       "0             1  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(features_json: Dict, feature: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a specified feature from a JSON object into a pandas DataFrame. The function expects the feature to be\n",
    "    represented as a dictionary within the JSON object, where each key-value pair corresponds to an entity's identifier\n",
    "    and its description, respectively. It creates a DataFrame with two columns: one for the identifiers (appending '_key'\n",
    "    to the feature name) and one for the descriptions (appending '_desc' to the feature name), ensuring that identifiers\n",
    "    are of integer type and descriptions are of string type.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The original DataFrame to be enriched.\n",
    "    enrichment_df : pd.DataFrame\n",
    "        The DataFrame containing additional information for the enrichment, with columns named 'feature_key' and 'feature_desc'.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The enriched DataFrame with additional descriptive information merged on the specified feature.\n",
    "    \"\"\"\n",
    "    feature_dict = features_json[feature]\n",
    "    df = pd.DataFrame.from_dict(\n",
    "        feature_dict,\n",
    "        columns=[\"val\"],\n",
    "        orient=\"index\").reset_index()\n",
    "    df.columns = [feature+\"_key\", feature+\"_desc\"]\n",
    "    df[feature+\"_key\"] = df[feature+\"_key\"].astype(int)\n",
    "    df[feature+\"_desc\"] = df[feature+\"_desc\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def enrich_df(df: pd.DataFrame, enrichment_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enriches a given DataFrame by merging it with another DataFrame that contains additional information for one of its\n",
    "    features. The function automatically identifies the feature to be enriched based on the naming convention used in the\n",
    "    'enrichment_df' (expects 'feature_key'). It performs a left merge on the specified feature, effectively adding the\n",
    "    descriptive information from 'enrichment_df' into 'df' and dropping the redundant key column from the merge.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The original DataFrame to be enriched.\n",
    "    enrichment_df : pd.DataFrame\n",
    "        The DataFrame containing additional information for the enrichment, with columns named 'feature_key' and 'feature_desc'.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The enriched DataFrame with additional descriptive information merged on the specified feature.\n",
    "    \"\"\"\n",
    "    feature = enrichment_df.columns[0].replace(\"_key\", \"\")\n",
    "    return df.merge(\n",
    "        enrichment_df,\n",
    "        how=\"left\",\n",
    "        left_on=feature,\n",
    "        right_on=feature+\"_key\"\n",
    "    ).drop(feature+\"_key\", axis=1)\n",
    "\n",
    "    # return pd.concat(\n",
    "    #     enrichment_df,\n",
    "    #     join=\"left\",\n",
    "    #     left_on=feature,\n",
    "    #     right_on=feature+\"_key\"\n",
    "    # ).drop(feature+\"_key\", axis=1)\n",
    "\n",
    "def remove_star_vals(df: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by removing rows where the description of a specified feature is marked with an asterisk (\"*\").\n",
    "    It calculates and prints the percentage of rows being removed because of this placeholder value. This step is crucial for\n",
    "    cleaning the dataset by eliminating entries with non-informative placeholder values, ensuring data quality for further\n",
    "    analysis or modeling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame to be cleaned.\n",
    "    feature : str\n",
    "        The feature whose descriptions are to be examined for placeholder values.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame excluding rows with placeholder descriptions for the specified feature.\n",
    "    \"\"\"\n",
    "    mask = df[feature+\"_desc\"] == \"*\"\n",
    "    print(f\"{feature} %rows: \" + str(len(df[mask]) / len(df) * 100))\n",
    "    return df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map(arr: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates a mapping dictionary from an array of sorted unique integers.\n",
    "\n",
    "    This function generates a symmetric mapping for the elements of the input array\n",
    "    based on their order. If the length of the array is even, it creates a mapping\n",
    "    such that the middle two elements map to -1 and 1, respectively, and other elements\n",
    "    are symmetrically mapped outwards with increasing absolute values. For an odd-length\n",
    "    array, the middle element is mapped to 1, and the remaining elements are symmetrically\n",
    "    mapped with increasing absolute values starting from 2.\n",
    "\n",
    "    The mapping is designed to transform the input array into a distribution centered\n",
    "    around 1, which could be useful for normalization purposes where a balanced around\n",
    "    one distribution is desired, such as in certain machine learning applications.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr : np.ndarray\n",
    "        A numpy array of sorted unique integers. The array should not contain any gaps\n",
    "        in integers and is expected to be sorted in ascending order.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict_map : Dict\n",
    "        A dictionary where keys are the original integers from the input array and values\n",
    "        are the integers mapped symmetrically around 1 as described above.\n",
    "\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> create_map(np.array([1, 2, 3, 4, 5]))\n",
    "    {-2: 1, -1: 2, 1: 3, 2: 4, 3: 5}\n",
    "\n",
    "    >>> create_map(np.array([1, 2, 3, 4]))\n",
    "    {-2: 1, -1: 2, 1: 3, 2: 4}\n",
    "\n",
    "    \"\"\"\n",
    "    aux_vals = np.arange(1, arr[-1] // 2 + 1)\n",
    "    if len(arr)%2 == 0:\n",
    "        aux_vals = np.concatenate([-np.flip(aux_vals), aux_vals])\n",
    "    else:\n",
    "        aux_vals = np.concatenate([-np.flip(aux_vals) - 1, np.array([1]), aux_vals + 1])\n",
    "    dict_map = {}\n",
    "    for ind, el in enumerate(arr):\n",
    "        dict_map[el] = aux_vals[ind]\n",
    "    return dict_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dict map is to reduce the happines\n",
    "# categories into three general classes of\n",
    "# unhappy, satisfied and happy\n",
    "aux_data = data[features_dict.keys()].copy()\n",
    "\n",
    "reduce_class_map = {\n",
    "    0: 0, 1: 0, 2: 0, 3: 0,\n",
    "    4: 1, 5: 1, 6: 1, 7: 1,\n",
    "    8: 2, 9: 2, 10: 2\n",
    "}\n",
    "\n",
    "aux_data[\"happy_reduced\"] = aux_data[\"happy\"].replace(reduce_class_map)\n",
    "aux_data = aux_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns for key 'stfmjob'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     aux_data\u001b[38;5;241m.\u001b[39mloc[aux_data[feature] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m55\u001b[39m, feature] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m66\u001b[39m\n\u001b[1;32m     11\u001b[0m aux_df \u001b[38;5;241m=\u001b[39m json_to_df(features_tables, feature)\n\u001b[0;32m---> 12\u001b[0m aux_data \u001b[38;5;241m=\u001b[39m \u001b[43menrich_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43maux_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m aux_data \u001b[38;5;241m=\u001b[39m remove_star_vals(aux_data, feature)\n\u001b[1;32m     15\u001b[0m mask \u001b[38;5;241m=\u001b[39m aux_data[feature\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[22], line 51\u001b[0m, in \u001b[0;36menrich_df\u001b[0;34m(df, enrichment_df)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mEnriches a given DataFrame by merging it with another DataFrame that contains additional information for one of its\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mfeatures. The function automatically identifies the feature to be enriched based on the naming convention used in the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    The enriched DataFrame with additional descriptive information merged on the specified feature.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m feature \u001b[38;5;241m=\u001b[39m enrichment_df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43menrichment_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(feature\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/how-happy-in-europe/lib/python3.10/site-packages/pandas/core/frame.py:10819\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10800\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10801\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10815\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10816\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10817\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10820\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10828\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/how-happy-in-europe/lib/python3.10/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/how-happy-in-europe/lib/python3.10/site-packages/pandas/core/reshape/merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 807\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/how-happy-in-europe/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1508\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1504\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1505\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1506\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1507\u001b[0m     ):\n\u001b[0;32m-> 1508\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on object and int64 columns for key 'stfmjob'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# rescale features from negative values to positive\n",
    "# values such that they are symmetric e.g. happiness\n",
    "# is from 0 to 10 we rescale it to -5 to 5\n",
    "# aux_data = data[features_dict.keys()].copy()\n",
    "features_map_dict = {}\n",
    "for feature in aux_data.columns:\n",
    "    if feature in ['cntry', \"happy_reduced\"]:\n",
    "        continue\n",
    "    if feature == \"wrklong\":\n",
    "        aux_data.loc[aux_data[feature] == 55, feature] = 66\n",
    "    aux_df = json_to_df(features_tables, feature)\n",
    "    aux_data = enrich_df(aux_data, aux_df)\n",
    "    aux_data = remove_star_vals(aux_data, feature)\n",
    "\n",
    "    mask = aux_data[feature+\"_desc\"] == \"-1\"\n",
    "    vals = np.sort(aux_data[~mask][feature].unique())\n",
    "    if 0 in vals:\n",
    "        aux_data.loc[~mask, feature] += 1\n",
    "        vals += 1\n",
    "    map_dict = create_map(vals)\n",
    "    features_map_dict[feature] = map_dict\n",
    "    aux_data[feature] = aux_data[feature].replace(map_dict)\n",
    "    aux_data.loc[mask, feature] = 0\n",
    "    if len(vals)%2 == 1:\n",
    "        indices = aux_data[aux_data[feature] == 1].index.to_list()\n",
    "        indices = np.random.choice(indices, len(indices)//2)\n",
    "        aux_data.loc[aux_data.index.isin(indices), feature] = -1\n",
    "aux_data = aux_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "categorical_features_indices = [aux_data.columns.get_loc(\"cntry\")]\n",
    "\n",
    "smote_nc = SMOTENC(categorical_features=categorical_features_indices, random_state=42)\n",
    "y_reduced = aux_data['happy_reduced']\n",
    "cols = [col for col in aux_data.columns if \"_desc\" not in col and \"happy\" not in col]\n",
    "X_res, y_res = smote_nc.fit_resample(aux_data[cols], aux_data[\"happy_reduced\"])\n",
    "aux_data = pd.concat([X_res, y_res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if SMOTENC balanced the categories\n",
    "aux_data[\"happy_reduced\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if everything is rescaled correctly\n",
    "for feature in aux_data.columns:\n",
    "    print(np.sort(aux_data[feature].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "cntry_encoded = encoder.fit_transform(aux_data[['cntry']])\n",
    "encoded_columns = [f\"cntry_{category}\" for category in encoder.categories_[0]]\n",
    "cntry_encoded_df = pd.DataFrame(cntry_encoded, columns=encoded_columns)\n",
    "aux_data = pd.concat([aux_data.drop(\"cntry\", axis=1), cntry_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(aux_data[\"happy_reduced\"].value_counts(), color=color);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show unique values of each happiness category\n",
    "np.sort(aux_data['happy_reduced'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax_X = MinMaxScaler()\n",
    "minmax_Y = MinMaxScaler()\n",
    "continuous_cols = [col for col in aux_data.columns if \"_desc\" not in col and \"happy\" not in col and \"cntry\" not in col]\n",
    "cntry_cols = [col for col in aux_data.columns if \"cntry\" in col]\n",
    "aux_data[continuous_cols] = minmax_X.fit_transform(aux_data[continuous_cols])\n",
    "aux_data[\"happy_reduced\"] = minmax_Y.fit_transform(aux_data[\"happy_reduced\"].values[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aux_data[continuous_cols + cntry_cols]\n",
    "y = aux_data[\"happy_reduced\"].values[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    booster=\"gbtree\",\n",
    "    n_jobs=10,\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the predictions\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_pred = np.round(minmax_Y.inverse_transform(y_pred[:, np.newaxis]))\n",
    "yy_test = minmax_Y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(yy_pred, yy_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(y_pred - y_test.squeeze(), kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%s/^[A-Z].*//g\n",
    "%s/\\(\\d\\) \\t/\\1: \"/g\n",
    "%s/^\\*.*//g\n",
    "%s/Not applicable\\*/-1/g\n",
    "%s/\".*/\\0\",/g\n",
    "%s/\".*\\*/\"\\*/g\n",
    "%s/^[a-z].*/}, \"\\0\": {/g\n",
    "%s/^\\n//g\n",
    "%s/,$\\n}/}/g\n",
    "%s/^\\(\\d\\+\\)/\"\\1\"/g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. KMeans vs Happiness classification\n",
    "2. ~Add country and gender with one hot encoding~\n",
    "3. Use GridSreachCV for cross-validation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-happy-in-europe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
