{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color = \"#507bbf\"\n",
    "plt.style.use(\"bmh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {\n",
    "    \"stfmjob\":  \"How satisfied are you in your main job\",\n",
    "    \"trdawrk\":  \"Too tired after work to enjoy things like doing at home, how often\",\n",
    "    \"jbprtfp\":  \"Job prevents you from giving time to partner/family, how often\",\n",
    "    \"pfmfdjba\": \"Partner/family fed up with pressure of your job, how often\",\n",
    "    \"dcsfwrka\": \"Current job: can decide time start/finish work\",\n",
    "    \"wrkhome\":  \"Work from home or place of choice, how often\",\n",
    "    \"wrklong\":  \"Employees expected to work overtime, how often\",\n",
    "    \"wrkresp\":  \"Employees expected to be responsive outside working hours, how often\",\n",
    "    \"health\":   \"Subjective general health\",\n",
    "    \"stfeco\":   \"How satisfied with present state of economy in country\",\n",
    "    \"hhmmb\":    \"Number of people living regularly as member of household\",\n",
    "    \"hincfel\":  \"Feeling about household's income nowadays\",\n",
    "    \"trstplc\":  \"Trust in the police\",\n",
    "    \"sclmeet\":  \"How often socially meet with friends, relatives or colleagues\",\n",
    "    \"hlthhmp\":  \"Hampered in daily activities by illness/disability/infirmity/mental problem\",\n",
    "    \"sclact\":   \"Take part in social activities compared to others of same age\",\n",
    "    \"iphlppl\":  \"Important to help people and care for others well-being\",\n",
    "    \"ipsuces\":  \"Important to be successful and that people recognise achievements\",\n",
    "    \"ipstrgv\":  \"Important that government is strong and ensures safety\",\n",
    "    \"gndr\"   :  \"Gender\",\n",
    "    \"cntry\"  :  \"Country\",\n",
    "    \"happy\":    \"Happiness\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"/home/mehrshad/code/arthurcornelio88/how-happy\"\n",
    "    +\"-in-europe/data/20240319_ESS10_manually-filtered_arthurcornelio88.csv\"\n",
    ").reset_index(drop=True)\n",
    "data = data.drop(\"Unnamed: 0\", axis=1)\n",
    "data = data[~data[\"happy\"].isin([77, 88, 99])]\n",
    "\n",
    "with open(\"../data/features_table.json\", 'r') as file:\n",
    "    features_tables = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(features_json: Dict, feature: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a specified feature from a JSON object into a pandas DataFrame. The function expects the feature to be\n",
    "    represented as a dictionary within the JSON object, where each key-value pair corresponds to an entity's identifier\n",
    "    and its description, respectively. It creates a DataFrame with two columns: one for the identifiers (appending '_key'\n",
    "    to the feature name) and one for the descriptions (appending '_desc' to the feature name), ensuring that identifiers\n",
    "    are of integer type and descriptions are of string type.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The original DataFrame to be enriched.\n",
    "    enrichment_df : pd.DataFrame\n",
    "        The DataFrame containing additional information for the enrichment, with columns named 'feature_key' and 'feature_desc'.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The enriched DataFrame with additional descriptive information merged on the specified feature.\n",
    "    \"\"\"\n",
    "    feature_dict = features_json[feature]\n",
    "    df = pd.DataFrame.from_dict(\n",
    "        feature_dict,\n",
    "        columns=[\"val\"],\n",
    "        orient=\"index\").reset_index()\n",
    "    df.columns = [feature+\"_key\", feature+\"_desc\"]\n",
    "    df[feature+\"_key\"] = df[feature+\"_key\"].astype(int)\n",
    "    df[feature+\"_desc\"] = df[feature+\"_desc\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def enrich_df(df: pd.DataFrame, enrichment_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enriches a given DataFrame by merging it with another DataFrame that contains additional information for one of its\n",
    "    features. The function automatically identifies the feature to be enriched based on the naming convention used in the\n",
    "    'enrichment_df' (expects 'feature_key'). It performs a left merge on the specified feature, effectively adding the\n",
    "    descriptive information from 'enrichment_df' into 'df' and dropping the redundant key column from the merge.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The original DataFrame to be enriched.\n",
    "    enrichment_df : pd.DataFrame\n",
    "        The DataFrame containing additional information for the enrichment, with columns named 'feature_key' and 'feature_desc'.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The enriched DataFrame with additional descriptive information merged on the specified feature.\n",
    "    \"\"\"\n",
    "    feature = enrichment_df.columns[0].replace(\"_key\", \"\")\n",
    "    return df.merge(\n",
    "        enrichment_df,\n",
    "        how=\"left\",\n",
    "        left_on=feature,\n",
    "        right_on=feature+\"_key\"\n",
    "    ).drop(feature+\"_key\", axis=1)\n",
    "\n",
    "def remove_star_vals(df: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by removing rows where the description of a specified feature is marked with an asterisk (\"*\").\n",
    "    It calculates and prints the percentage of rows being removed because of this placeholder value. This step is crucial for\n",
    "    cleaning the dataset by eliminating entries with non-informative placeholder values, ensuring data quality for further\n",
    "    analysis or modeling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame to be cleaned.\n",
    "    feature : str\n",
    "        The feature whose descriptions are to be examined for placeholder values.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame excluding rows with placeholder descriptions for the specified feature.\n",
    "    \"\"\"\n",
    "    mask = df[feature+\"_desc\"] == \"*\"\n",
    "    print(f\"{feature} %rows: \" + str(len(df[mask]) / len(df) * 100))\n",
    "    return df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map(arr: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates a mapping dictionary from an array of sorted unique integers.\n",
    "\n",
    "    This function generates a symmetric mapping for the elements of the input array\n",
    "    based on their order. If the length of the array is even, it creates a mapping\n",
    "    such that the middle two elements map to -1 and 1, respectively, and other elements\n",
    "    are symmetrically mapped outwards with increasing absolute values. For an odd-length\n",
    "    array, the middle element is mapped to 1, and the remaining elements are symmetrically\n",
    "    mapped with increasing absolute values starting from 2.\n",
    "\n",
    "    The mapping is designed to transform the input array into a distribution centered\n",
    "    around 1, which could be useful for normalization purposes where a balanced around\n",
    "    one distribution is desired, such as in certain machine learning applications.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr : np.ndarray\n",
    "        A numpy array of sorted unique integers. The array should not contain any gaps\n",
    "        in integers and is expected to be sorted in ascending order.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict_map : Dict\n",
    "        A dictionary where keys are the original integers from the input array and values\n",
    "        are the integers mapped symmetrically around 1 as described above.\n",
    "\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> create_map(np.array([1, 2, 3, 4, 5]))\n",
    "    {-2: 1, -1: 2, 1: 3, 2: 4, 3: 5}\n",
    "\n",
    "    >>> create_map(np.array([1, 2, 3, 4]))\n",
    "    {-2: 1, -1: 2, 1: 3, 2: 4}\n",
    "\n",
    "    \"\"\"\n",
    "    aux_vals = np.arange(1, arr[-1] // 2 + 1)\n",
    "    if len(arr)%2 == 0:\n",
    "        aux_vals = np.concatenate([-np.flip(aux_vals), aux_vals])\n",
    "    else:\n",
    "        aux_vals = np.concatenate([-np.flip(aux_vals) - 1, np.array([1]), aux_vals + 1])\n",
    "    dict_map = {}\n",
    "    for ind, el in enumerate(arr):\n",
    "        dict_map[el] = aux_vals[ind]\n",
    "    return dict_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dict map is to reduce the happines\n",
    "# categories into three general classes of\n",
    "# unhappy, satisfied and happy\n",
    "aux_data = data[features_dict.keys()].copy()\n",
    "\n",
    "reduce_class_map = {\n",
    "    0: 0, 1: 0, 2: 0, 3: 0,\n",
    "    4: 1, 5: 1, 6: 1, 7: 1,\n",
    "    8: 2, 9: 2, 10: 2\n",
    "}\n",
    "\n",
    "aux_data[\"happy_reduced\"] = aux_data[\"happy\"].replace(reduce_class_map)\n",
    "aux_data = aux_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_indices = [aux_data.columns.get_loc(\"cntry\")]\n",
    "categorical_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "smote_nc = SMOTENC(categorical_features=categorical_features_indices, random_state=42)\n",
    "y_reduced = aux_data['happy_reduced']\n",
    "cols = [col for col in aux_data.columns if \"_desc\" not in col and \"happy\" not in col]\n",
    "X_res, y_res = smote_nc.fit_resample(aux_data[cols], aux_data[\"happy_reduced\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_data = pd.concat([X_res, y_res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale features from negative values to positive\n",
    "# values such that they are symmetric e.g. happiness\n",
    "# is from 0 to 10 we rescale it to -5 to 5\n",
    "# aux_data = data[features_dict.keys()].copy()\n",
    "features_map_dict = {}\n",
    "for feature in aux_data.columns:\n",
    "    if feature in ['cntry', \"happy_reduced\"]:\n",
    "        continue\n",
    "    if feature == \"wrklong\":\n",
    "        aux_data.loc[aux_data[feature] == 55, feature] = 66\n",
    "    aux_df = json_to_df(features_tables, feature)\n",
    "    aux_data = enrich_df(aux_data, aux_df)\n",
    "    aux_data = remove_star_vals(aux_data, feature)\n",
    "\n",
    "    mask = aux_data[feature+\"_desc\"] == \"-1\"\n",
    "    vals = np.sort(aux_data[~mask][feature].unique())\n",
    "    if 0 in vals:\n",
    "        aux_data.loc[~mask, feature] += 1\n",
    "        vals += 1\n",
    "    map_dict = create_map(vals)\n",
    "    features_map_dict[feature] = map_dict\n",
    "    aux_data[feature] = aux_data[feature].replace(map_dict)\n",
    "    aux_data.loc[mask, feature] = 0\n",
    "    if len(vals)%2 == 1:\n",
    "        indices = aux_data[aux_data[feature] == 1].index.to_list()\n",
    "        indices = np.random.choice(indices, len(indices)//2)\n",
    "        aux_data.loc[aux_data.index.isin(indices), feature] = -1\n",
    "aux_data = aux_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_data['happy_reduced'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "cntry_encoded = encoder.fit_transform(aux_data[['cntry']])\n",
    "encoded_columns = [f\"cntry_{category}\" for category in encoder.categories_[0]]\n",
    "cntry_encoded_df = pd.DataFrame(cntry_encoded, columns=encoded_columns)\n",
    "aux_data = pd.concat([aux_data.drop(\"cntry\", axis=1), cntry_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(aux_data[\"happy_reduced\"].value_counts(), color=color);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(aux_data['happy_reduced'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax_X = MinMaxScaler()\n",
    "minmax_Y = MinMaxScaler()\n",
    "continuous_cols = [col for col in aux_data.columns if \"_desc\" not in col and \"happy\" not in col and \"cntry\" not in col]\n",
    "cntry_cols = [col for col in aux_data.columns if \"cntry\" in col]\n",
    "aux_data[continuous_cols] = minmax_X.fit_transform(aux_data[continuous_cols])\n",
    "aux_data[\"happy_reduced\"] = minmax_Y.fit_transform(aux_data[\"happy_reduced\"].values[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aux_data[continuous_cols + cntry_cols]\n",
    "y = aux_data[\"happy_reduced\"].values[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    booster=\"gbtree\",\n",
    "    n_jobs=10,\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the predictions\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_pred = np.round(minmax_Y.inverse_transform(y_pred[:, np.newaxis]))\n",
    "yy_test = minmax_Y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(yy_pred, yy_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(y_pred - y_test.squeeze(), kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%s/^[A-Z].*//g\n",
    "%s/\\(\\d\\) \\t/\\1: \"/g\n",
    "%s/^\\*.*//g\n",
    "%s/Not applicable\\*/-1/g\n",
    "%s/\".*/\\0\",/g\n",
    "%s/\".*\\*/\"\\*/g\n",
    "%s/^[a-z].*/}, \"\\0\": {/g\n",
    "%s/^\\n//g\n",
    "%s/,$\\n}/}/g\n",
    "%s/^\\(\\d\\+\\)/\"\\1\"/g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. KMeans vs Happiness classification\n",
    "2. ~Add country and gender with one hot encoding~\n",
    "3. Use GridSreachCV for cross-validation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-happy-in-europe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
